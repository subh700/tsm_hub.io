<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LSTM Model - TSM Hub</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
</head>
<body class="bg-slate-100 text-slate-800">

    <header class="bg-white shadow-md sticky top-0 z-50">
        <nav class="container mx-auto px-6 py-4 flex justify-between items-center">
            <a href="index.html" class="text-2xl font-bold text-slate-900">TSM Hub</a>
            <div class="hidden md:flex space-x-6 items-center">
                <a href="index.html" class="nav-link">Home</a>
                <a href="concepts.html" class="nav-link">Concepts</a>
                <a href="models.html" class="nav-link active">Models</a>
                <a href="comparison.html" class="nav-link">Comparison</a>
            </div>
            <button id="mobile-menu-button" class="md:hidden p-2">
                <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7" />
                </svg>
            </button>
        </nav>
        <div id="mobile-menu" class="hidden md:hidden">
            <a href="index.html" class="block py-2 px-4 text-sm hover:bg-slate-200">Home</a>
            <a href="concepts.html" class="block py-2 px-4 text-sm hover:bg-slate-200">Concepts</a>
            <a href="models.html" class="block py-2 px-4 text-sm hover:bg-slate-200">Models</a>
            <a href="comparison.html" class="block py-2 px-4 text-sm hover:bg-slate-200">Comparison</a>
        </div>
    </header>

    <main class="container mx-auto p-4 md:p-8">
        <div class="bg-white p-8 rounded-lg shadow-md">
            <header class="border-b-2 border-slate-200 pb-6 mb-8">
                 <a href="models.html" class="text-blue-600 hover:underline mb-4 inline-block">&larr; Back to Model Library</a>
                <h1 class="text-4xl md:text-5xl font-bold text-slate-900">LSTM Model</h1>
                <p class="mt-2 text-lg text-slate-600">Long Short-Term Memory Network</p>
            </header>
            
            <article class="prose max-w-none">
                <section>
                    <h2>Overview</h2>
                    <p>Long Short-Term Memory (LSTM) networks are a specialized type of Recurrent Neural Network (RNN) designed to overcome the limitations of traditional RNNs in learning long-term dependencies. Standard RNNs suffer from the vanishing gradient problem, which makes it difficult for them to remember information over long sequences. LSTMs introduce a "memory cell" and gating mechanisms to control the flow of information, allowing them to selectively remember or forget information over extended periods.</p>
                    
                    <h2>Architecture & Components</h2>
                    <p>The core innovation of the LSTM is the memory cell ($C_t$), which acts as a conveyor belt of information through the sequence. The flow of information into and out of this cell is regulated by three main "gates":</p>
                    <ul>
                        <li><strong>Forget Gate ($f_t$):</strong> This gate decides what information to discard from the cell state. It looks at the previous hidden state ($h_{t-1}$) and the current input ($x_t$) and outputs a number between 0 and 1 for each number in the previous cell state $C_{t-1}$. A 1 means "completely keep this" while a 0 means "completely get rid of this."</li>
                        <li><strong>Input Gate ($i_t$):</strong> This gate decides which new information to store in the cell state. It consists of two parts: a sigmoid layer that decides which values to update, and a tanh layer that creates a vector of new candidate values, $\tilde{C}_t$, that could be added to the state.</li>
                        <li><strong>Output Gate ($o_t$):</strong> This gate determines the next hidden state, which is a filtered version of the cell state. First, a sigmoid layer decides which parts of the cell state weâ€™re going to output. Then, we put the cell state through a tanh function (to push the values to be between -1 and 1) and multiply it by the output of the sigmoid gate.</li>
                    </ul>
                     <div class="bg-slate-100 p-4 rounded-lg my-6 text-center">
                        <img src="https://placehold.co/600x300/e2e8f0/334155?text=Diagram+of+LSTM+Cell" alt="LSTM Cell Diagram" class="mx-auto rounded-md">
                        <p class="text-sm text-slate-500 mt-2">Conceptual diagram of an LSTM cell showing the forget, input, and output gates controlling the cell state.</p>
                    </div>
                </section>
                
                <section>
                    <h2>When to Use LSTM</h2>
                    <p>LSTMs are a powerful choice for complex time series problems, especially when:</p>
                    <ul>
                        <li>The data contains complex, non-linear patterns that classical models cannot capture.</li>
                        <li>There are long-term dependencies in the data (e.g., the current value depends on events that happened many time steps ago).</li>
                        <li>You have a large amount of training data. Deep learning models like LSTMs typically require more data than classical statistical models.</li>
                        <li>Performance is more critical than model interpretability. While techniques exist to interpret LSTMs, they are inherently more of a "black box" than models like ARIMA.</li>
                    </ul>
                </section>

                <section>
                    <h2>Example Implementation</h2>
                    <p>Here's a conceptual implementation of an LSTM model for time series forecasting using TensorFlow/Keras. The process involves reshaping the data into sequences of inputs and corresponding outputs, scaling the data, and then building and training the network.</p>
                    <div class="code-block my-4">
                        <pre class="bg-yellow-100 rounded p-4 text-sm mb-2 overflow-x-auto">
# Import necessary libraries
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler

# Generate sample data with non-linear patterns
np.random.seed(42)
n_samples = 200
time = np.arange(n_samples)
data = np.sin(time / 20) * 10 + time * 0.1 + np.random.randn(n_samples) * 2
data = data.reshape(-1, 1)

# Scale data to be between 0 and 1
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

# Create sequences for LSTM
def create_dataset(dataset, look_back=1):
    X, Y = [], []
    for i in range(len(dataset) - look_back - 1):
        a = dataset[i:(i + look_back), 0]
        X.append(a)
        Y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(Y)

look_back = 10
X, y = create_dataset(scaled_data, look_back)

# Reshape input to be [samples, time steps, features]
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

# Build the LSTM model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(look_back, 1)))
model.add(LSTM(50))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(X, y, epochs=20, batch_size=1, verbose=2)

# For prediction, you would take the last 'look_back' points from the training data,
# predict the next point, append it, and repeat.
print("LSTM model training complete.")
                        </pre>
                    </div>
                </section>
            </article>
        </div>
    </main>

    <footer class="text-center p-6 bg-slate-200 mt-12">
        <p class="text-slate-600 text-sm">TSM Hub: An Interactive Resource for Time Series Analysis</p>
    </footer>

    <script src="main.js"></script>
</body>
</html>
