<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gradient Boosting Regressor Model - TSM Hub</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
</head>
<body class="bg-slate-100 text-slate-800">
<!-- Enhanced Navigation for Model Pages -->
<nav class="bg-white shadow-lg sticky top-0 z-50 mb-8">
    <div class="container mx-auto px-6 py-4">
        <div class="flex justify-between items-center">
            <div class="flex items-center space-x-4">
                <a href="index.html" class="text-2xl font-bold text-blue-600">üè† TSF Hub</a>
                <div class="hidden md:flex items-center space-x-1 bg-gray-100 rounded-lg p-1">
                    <a href="models.html" class="nav-link px-3 py-2 rounded">üìö All Models</a>
                    <a href="comparison.html" class="nav-link px-3 py-2 rounded">üìä Compare</a>
                </div>
            </div>
            
            <div class="flex items-center space-x-3">
                <!-- AI Features Quick Access -->
                <div class="hidden md:flex items-center space-x-2">
                    <a href="model-selector.html" class="bg-blue-500 text-white px-3 py-2 rounded-lg hover:bg-blue-600 text-sm flex items-center">
                        ü§ñ AI Recommender
                    </a>
                    <a href="chat.html" class="bg-green-500 text-white px-3 py-2 rounded-lg hover:bg-green-600 text-sm flex items-center">
                        üí¨ AI Assistant
                    </a>
                    <a href="forecast-simulator.html?model={{MODEL_NAME}}" class="bg-purple-500 text-white px-3 py-2 rounded-lg hover:bg-purple-600 text-sm flex items-center">
                        üöÄ Try in Simulator
                    </a>
                </div>
                
                <!-- Mobile menu button -->
                <button class="md:hidden p-2" onclick="toggleMobileMenu()">‚ò∞</button>
            </div>
        </div>
        
        <!-- Mobile Menu -->
        <div id="mobile-menu" class="hidden md:hidden mt-4 pb-4 border-t">
            <div class="flex flex-col space-y-2">
                <a href="models.html" class="text-gray-600 hover:text-blue-600 py-2">üìö All Models</a>
                <a href="comparison.html" class="text-gray-600 hover:text-blue-600 py-2">üìä Compare Models</a>
                <a href="model-selector.html" class="text-gray-600 hover:text-blue-600 py-2">ü§ñ AI Recommender</a>
                <a href="chat.html" class="text-gray-600 hover:text-blue-600 py-2">üí¨ AI Assistant</a>
                <a href="forecast-simulator.html" class="text-gray-600 hover:text-blue-600 py-2">üöÄ Simulator</a>
            </div>
        </div>
    </div>
</nav>

    <header class="bg-white shadow-md sticky top-0 z-50">
        <nav class="container mx-auto px-6 py-4 flex justify-between items-center">
            <a href="index.html" class="text-2xl font-bold text-slate-900">TSM Hub</a>
            <div class="hidden md:flex space-x-6 items-center">
                <a href="index.html" class="nav-link">Home</a>
                <a href="concepts.html" class="nav-link">Concepts</a>
                <a href="models.html" class="nav-link active">Models</a>
                <a href="comparison.html" class="nav-link">Comparison</a>
            </div>
            <button id="mobile-menu-button" class="md:hidden p-2">
                <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7" />
                </svg>
            </button>
        </nav>
        <div id="mobile-menu" class="hidden md:hidden">
            <a href="index.html" class="block py-2 px-4 text-sm hover:bg-slate-200">Home</a>
            <a href="concepts.html" class="block py-2 px-4 text-sm hover:bg-slate-200">Concepts</a>
            <a href="models.html" class="block py-2 px-4 text-sm hover:bg-slate-200">Models</a>
            <a href="comparison.html" class="block py-2 px-4 text-sm hover:bg-slate-200">Comparison</a>
        </div>
    </header>

    <main class="container mx-auto p-4 md:p-8">
        <div class="bg-white p-8 rounded-lg shadow-md">
            <header class="border-b-2 border-slate-200 pb-6 mb-8">
                 <a href="models.html" class="text-blue-600 hover:underline mb-4 inline-block">&larr; Back to Model Library</a>
                <h1 class="text-4xl md:text-5xl font-bold text-slate-900">Gradient Boosting Regressor Model</h1>
                <p class="mt-2 text-lg text-slate-600">Powerful Ensemble for Regression</p>
            </header>
            
            <article class="prose max-w-none">
                <section>
                    <h2>Overview</h2>
                    <p>Gradient Boosting Regressor (GBR) is a powerful ensemble machine learning technique that builds an additive model in a forward stage-wise fashion. It iteratively trains weak learners (typically decision trees) to correct the errors of previous learners. GBR is highly effective for both regression and classification problems and has gained significant popularity due to its ability to achieve excellent results across a wide range of use cases. For time series forecasting, GBR can be adapted by transforming the time series problem into a supervised learning problem through **feature engineering**.</p>
                    
                    <h2>Architecture & Components</h2>
                    <p>The core idea of Gradient Boosting is to combine many weak learners to form a strong learner. Its key components include:</p>
                    <ul>
                        <li><strong>Weak Learners:</strong> Typically, shallow decision trees (e.g., `max_depth` of 3-5) are used as weak learners.</li>
                        <li><strong>Additive Model:</strong> The final prediction is the sum of predictions from all individual trees.
                            <p class="font-mono text-center bg-slate-100 p-2 rounded-md mathjax-process">
                                $ F_m(x) = F_{m-1}(x) + \gamma_m h_m(x) $
                            </p>
                            Where $F_m(x)$ is the ensemble model at step $m$, $h_m(x)$ is the new weak learner, and $\gamma_m$ is its weight.
                        </li>
                        <li><strong>Loss Function:</strong> GBR minimizes a differentiable loss function (e.g., mean squared error for regression) by iteratively moving towards its minimum. At each step, a new tree is fitted to the negative gradient of the loss function with respect to the current predictions (i.e., the residuals).</li>
                        <li><strong>Learning Rate (Shrinkage):</strong> A learning rate (or shrinkage parameter) is applied to the contribution of each tree. This reduces the step size at each iteration, helping to prevent overfitting and improve generalization.</li>
                        <li><strong>Subsampling (Stochastic Gradient Boosting):</strong> A variant where each tree is trained on a random subset of the training data. This further reduces variance and improves robustness.</li>
                        <li><strong>Feature Engineering:</strong> For time series forecasting, GBR relies on manually engineered features to capture temporal patterns. These typically include:
                            <ul>
                                <li><strong>Lagged Features:</strong> Past values of the time series itself.</li>
                                <li><strong>Rolling Window Statistics:</strong> Mean, standard deviation, min, max over a defined past window.</li>
                                <li><strong>Time-Based Features:</strong> Day of week, month, year, hour, quarter, and holiday indicators. [23]</li>
                                <li><strong>Decomposition:</strong> Explicitly decomposing the time series into trend, seasonality, and residuals, and then using these components as features or training GBR on the residuals. [24]</li>
                            </ul>
                        </li>
                    </ul>
                     <div class="bg-slate-100 p-4 rounded-lg my-6 text-center">
                        <img src="https://placehold.co/600x300/e2e8f0/334155?text=Conceptual+Gradient+Boosting+for+Time+Series" alt="Gradient Boosting Regressor Architecture Diagram" class="mx-auto rounded-md">
                        <p class="text-sm text-slate-500 mt-2">Conceptual diagram of Gradient Boosting Regressor's iterative tree-building process for time series.</p>
                    </div>
                </section>
                
                <section>
                    <h2>When to Use Gradient Boosting Regressor</h2>
                    <p>GBR is a powerful choice for time series forecasting when:</p>
                    <ul>
                        <li><strong>High predictive accuracy is paramount:</strong> It often provides superior accuracy compared to other regression techniques.</li>
                        <li><strong>You are comfortable with feature engineering:</strong> Its effectiveness in time series relies on creating relevant temporal features.</li>
                        <li><strong>The time series exhibits complex non-linear relationships:</strong> It can capture intricate interactions between features.</li>
                        <li><strong>You need to model both trend and seasonality:</strong> Through appropriate feature engineering or decomposition.</li>
                        <li><strong>Robustness to different data types is important:</strong> It can handle various types of input features.</li>
                        <li><strong>You need to optimize for specific horizons:</strong> It can be more flexible for varying forecast horizons compared to some traditional models. [5]</li>
                    </ul>
                </section>

                <section>
                    <h2>Pros and Cons</h2>
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                        <div>
                            <h3 class="text-lg font-semibold mb-2 text-green-600">Pros</h3>
                            <ul class="list-disc list-inside text-slate-600 space-y-1">
                                <li><strong>High Predictive Accuracy:</strong> Generally provides excellent accuracy, often outperforming many other regression techniques.</li>
                                <li><strong>Handles Complex Non-Linear Relationships:</strong> Capable of modeling intricate interactions between features.</li>
                                <li><strong>Flexible Loss Functions:</strong> Can incorporate various loss functions, including quantile loss for prediction intervals.</li>
                                <li><strong>Robust to Different Data Types:</strong> Can handle numerical and categorical features.</li>
                                <li><strong>Provides Feature Importance:</strong> Can identify which features contribute most to predictions.</li>
                            </ul>
                        </div>
                        <div>
                            <h3 class="text-lg font-semibold mb-2 text-red-600">Cons</h3>
                            <ul class="list-disc list-inside text-slate-600 space-y-1">
                                <li><strong>Requires Feature Engineering:</strong> Not a native time series model; requires manual creation of lagged, rolling, and time-based features.</li>
                                <li><strong>Struggles with Extrapolation:</strong> As a tree-based model, it cannot predict values outside the range seen in the training data.</li>
                                <li><strong>Computationally Intensive:</strong> Sequential nature of boosting can make training slow for very large datasets.</li>
                                <li><strong>Prone to Overfitting:</strong> Can overfit if not tuned carefully, especially with many boosting stages or deep trees.</li>
                                <li><strong>Less Interpretable:</strong> Ensemble of many trees makes it less interpretable than simpler models.</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>Example Implementation</h2>
                    <p>Here's an example of implementing Gradient Boosting Regressor for time series forecasting in Python using `scikit-learn`. The key steps involve feature engineering, chronological data splitting, and then training and predicting with `GradientBoostingRegressor`.</p>

                    <h3>Python Example (using `scikit-learn` library)</h3>
                    <pre class="bg-yellow-100 rounded p-4 text-sm mb-2 overflow-x-auto">
                        <code class="language-python">
import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt

# 1. Create sample time series data
date_range = pd.date_range(start='2020-01-01', periods=300, freq='D')
# Simulate data with trend, seasonality, and noise
values = (100 + np.arange(300) * 0.5 + # Trend
          20 * np.sin(np.arange(300) * 2 * np.pi / 30) + # Monthly seasonality
          np.random.randn(300) * 5) # Noise
df = pd.DataFrame({'date': date_range, 'value': values})

# 2. Feature Engineering (Create lagged and time-based features)
def create_features(df):
    df['lag_1'] = df['value'].shift(1)
    df['lag_7'] = df['value'].shift(7) # Weekly lag
    df['rolling_mean_7'] = df['value'].rolling(window=7).mean().shift(1)
    df['day_of_week'] = df['date'].dt.dayofweek
    df['month'] = df['date'].dt.month
    df['year'] = df['date'].dt.year
    df['day_of_year'] = df['date'].dt.dayofyear
    return df

df = create_features(df.copy())

# Drop rows with NaN values created by lagging/rolling features
df = df.dropna()

# 3. Splitting Data (Chronological Split)
split_date = '2020-09-01'
train = df[df['date'] < split_date]
test = df[df['date'] >= split_date]

features = [col for col in df.columns if col not in ['date', 'value']]
target = 'value'

X_train, y_train = train[features], train[target]
X_test, y_test = test[features], test[target]

# 4. Create and Train Gradient Boosting Regressor Model
# n_estimators: number of boosting stages
# learning_rate: shrinks the contribution of each tree
# max_depth: limits the number of nodes in the tree
reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
reg.fit(X_train, y_train)

# 5. Make Predictions
predictions = reg.predict(X_test)

# 6. Evaluate Model Performance
mae = mean_absolute_error(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))
print(f"MAE: {mae:.3f}")
print(f"RMSE: {rmse:.3f}")

# 7. Plotting Results
plt.figure(figsize=(14, 7))
plt.plot(train['date'], train['value'], label='Training Data', color='blue')
plt.plot(test['date'], y_test, label='Actual Test Data', color='orange')
plt.plot(test['date'], predictions, label='GBR Predictions', color='green', linestyle='--')
plt.title('Gradient Boosting Regressor Time Series Forecasting')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.grid(True)
plt.show()
                        </code></pre>
                    </div>
                </section>
                
                <section>
                    <h2>Dependencies & Resources</h2>
                    <p><strong>Dependencies:</strong> <code>pandas</code>, <code>numpy</code>, <code>scikit-learn</code>, <code>matplotlib</code> (for plotting).</p>
                    <ul class="list-disc list-inside space-y-2">
                        <li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html" target="_blank" rel="noopener noreferrer">Scikit-learn GradientBoostingRegressor Documentation ‚Üó</a></li>
                        <li><a href="https://www.digitalocean.com/community/tutorials/implementing-gradient-boosting-regression-python" target="_blank" rel="noopener noreferrer">Implementing Gradient Boosting Regression in Python (DigitalOcean) ‚Üó</a></li>
                        <li><a href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html" target="_blank" rel="noopener noreferrer">Prediction Intervals for Gradient Boosting Regression (Scikit-learn Example) ‚Üó</a></li>
                        <li><a href="https://www.analyticsvidhya.com/blog/2020/09/alternative-hyperparameter-optimization-technique-you-need-to-know-hyperopt/" target="_blank" rel="noopener noreferrer">Alternative Hyperparameter Optimization Technique (Analytics Vidhya) ‚Üó</a> [25]</li>
                        <li><a href="https://wandb.ai/iamleonie/A-Gentle-Introduction-to-Time-Series-Analysis-Forecasting/reports/A-gentle-introduction-to-time-series-analysis-forecasting--VmlldzoxMDg5NDMxMw" target="_blank" rel="noopener noreferrer">A Gentle Introduction to Time Series Analysis & Forecasting (WandB Report) ‚Üó</a> [24]</li>
                        <li><a href="https://rstudio-pubs-static.s3.amazonaws.com/161075_05ce98dc51c844e0833c06835c9ce4c3.html" target="_blank" rel="noopener noreferrer">Time series forecast with stochastic gradient boosting (RStudio Pubs) ‚Üó</a> [23]</li>
                    </ul>
                </section>
            </article>
        </div>
    </main>

    <footer class="text-center p-6 bg-slate-200 mt-12">
        <p class="text-slate-600 text-sm">TSM Hub: An Interactive Resource for Time Series Analysis</p>
    </footer>

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$','$'], ['\\(','\\)']],
                displayMath: [['$$','$$'], ['\\[','\\]']],
                processEscapes: true,
                ignoreHtmlClass: ".*",
                processHtmlClass: "mathjax-process"
            },
            skipStartupTypeset: true
        });
    </script>
    <script src="main.js"></script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script>
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof MathJax!== 'undefined') {
                MathJax.Hub.Queue();
            }
        });
    </script>
</body>
</html>
