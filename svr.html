<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SVR Model - TSM Hub</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
</head>
<body class="bg-slate-100 text-slate-800">
<!-- Enhanced Navigation for Model Pages -->


    <header class="bg-white shadow-md sticky top-0 z-50">
        <nav class="container mx-auto px-6 py-4 flex justify-between items-center">
            <a href="index.html" class="text-2xl font-bold text-slate-900">üè† TSF Hub</a>
            <div class="hidden md:flex space-x-6 items-center">
                <a href="index.html" class="nav-link">Home</a>
                <a href="concepts.html" class="nav-link">Concepts</a>
                <a href="models.html" class="nav-link active">Models</a>
                <a href="comparison.html" class="nav-link">Comparison</a>
            </div>
            <button id="mobile-menu-button" class="md:hidden p-2">
                <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7" />
                </svg>
            </button>
        </nav>
        <div id="mobile-menu" class="hidden md:hidden">
            <a href="index.html" class="block py-2 px-4 text-sm hover:bg-slate-200">Home</a>
            <a href="concepts.html" class="block py-2 px-4 text-sm hover:bg-slate-200">Concepts</a>
            <a href="models.html" class="block py-2 px-4 text-sm hover:bg-slate-200">Models</a>
            <a href="comparison.html" class="block py-2 px-4 text-sm hover:bg-slate-200">Comparison</a>
        </div>
    </header>

    <main class="container mx-auto p-4 md:p-8">
        <div class="bg-white p-8 rounded-lg shadow-md">
            <header class="border-b-2 border-slate-200 pb-6 mb-8">
                 <a href="models.html" class="text-blue-600 hover:underline mb-4 inline-block">&larr; Back to Model Library</a>
                <h1 class="text-4xl md:text-5xl font-bold text-slate-900">SVR Model</h1>
                <p class="mt-2 text-lg text-slate-600">Support Vector Regression</p>
            </header>
            
            <article class="prose max-w-none">
                <section>
                    <h2>Overview</h2>
                    <p>Support Vector Regression (SVR) is a supervised learning model that uses the principles of Support Vector Machines (SVMs) to perform regression analysis. Unlike traditional regression models that aim to minimize the squared error, SVR aims to find a function that deviates from the true targets by a margin $\epsilon$ (epsilon) at most, while being as flat (simple) as possible. This "epsilon-insensitive" loss function makes SVR robust to outliers. SVR can be applied to time series forecasting by transforming the time series problem into a supervised learning problem through **feature engineering**.</p>
                    
                    <h2>Architecture & Components</h2>
                    <p>SVR's architecture is based on finding an optimal hyperplane in a high-dimensional feature space:</p>
                    <ul>
                        <li><strong>Kernel Function:</strong> SVR maps the input data into a high-dimensional feature space using a kernel function (e.g., Radial Basis Function (RBF), linear, polynomial, sigmoid). This allows SVR to model non-linear relationships in the original input space by performing linear regression in the transformed high-dimensional space.
                            <p class="font-mono text-center bg-slate-100 p-2 rounded-md mathjax-process">
                                $ f(x) = \langle w, \Phi(x) \rangle + b $
                            </p>
                            Where $\Phi(x)$ is the non-linear transformation to the high-dimensional feature space.
                        </li>
                        <li><strong>$\epsilon$-Insensitive Loss Function:</strong> SVR introduces an $\epsilon$-tube around the regression line. Errors within this tube are not penalized, making the model robust to small errors and outliers. The objective is to minimize the error outside this $\epsilon$-tube.
                            <p class="font-mono text-center bg-slate-100 p-2 rounded-md mathjax-process">
                                $ \text{minimize } \frac{1}{2} ||w||^2 + C \sum (\xi_i + \xi_i^*) $ <br>
                                $ \text{subject to } y_i - \langle w, \Phi(x_i) \rangle - b \le \epsilon + \xi_i $ <br>
                                $ \langle w, \Phi(x_i) \rangle + b - y_i \le \epsilon + \xi_i^* $ <br>
                                $ \xi_i, \xi_i^* \ge 0 $
                            </p>
                            Where $C$ is a regularization parameter that controls the trade-off between model flatness and error tolerance, and $\xi_i, \xi_i^*$ are slack variables for errors outside the $\epsilon$-tube.
                        </li>
                        <li><strong>Support Vectors:</strong> Only a subset of the training data points, called "support vectors," influence the final model. These are the points that lie on or outside the $\epsilon$-tube.</li>
                        <li><strong>Feature Engineering:</strong> For time series forecasting, SVR relies on manually engineered features to capture temporal patterns. These typically include:
                            <ul>
                                <li><strong>Lagged Features:</strong> Past values of the time series itself. [18]</li>
                                <li><strong>Time-Based Features:</strong> Day of week, month, year, etc.</li>
                                <li><strong>Rolling Statistics:</strong> Moving averages, standard deviations.</li>
                                <li><strong>Decomposition:</strong> Separating trend, seasonality, and random effects to increase forecast accuracy. [19]</li>
                            </ul>
                        </li>
                    </ul>
                     <div class="bg-slate-100 p-4 rounded-lg my-6 text-center">
                        <img src="https://placehold.co/600x300/e2e8f0/334155?text=Conceptual+SVR+for+Time+Series" alt="SVR Architecture Diagram" class="mx-auto rounded-md">
                        <p class="text-sm text-slate-500 mt-2">Conceptual diagram of SVR's $\epsilon$-insensitive tube and hyperplane for time series.</p>
                    </div>
                </section>
                
                <section>
                    <h2>When to Use SVR</h2>
                    <p>SVR can be a suitable choice for time series forecasting when:</p>
                    <ul>
                        <li><strong>Non-linear relationships are present:</strong> Its use of kernel functions allows it to model complex non-linear patterns.</li>
                        <li><strong>Robustness to outliers is important:</strong> The $\epsilon$-insensitive loss function makes it less sensitive to extreme values.</li>
                        <li><strong>The dataset size is moderate:</strong> SVR can be computationally intensive for very large datasets.</li>
                        <li><strong>You are comfortable with feature engineering:</strong> It requires transforming the time series into a supervised learning problem.</li>
                        <li><strong>High-dimensional feature spaces are involved:</strong> It performs well in such spaces.</li>
                        <li><strong>You need a model that avoids overfitting by adopting structural risk minimization.</strong> [19]</li>
                    </ul>
                </section>

                <section>
                    <h2>Pros and Cons</h2>
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                        <div>
                            <h3 class="text-lg font-semibold mb-2 text-green-600">Pros</h3>
                            <ul class="list-disc list-inside text-slate-600 space-y-1">
                                <li><strong>Handles Non-Linear Relationships:</strong> Through various kernel functions.</li>
                                <li><strong>Robust to Outliers:</strong> Due to the $\epsilon$-insensitive loss function.</li>
                                <li><strong>Avoids Overfitting:</strong> By minimizing the structural risk, not just empirical error.</li>
                                <li><strong>Effective in High-Dimensional Spaces:</strong> Can perform well even when the number of features is large.</li>
                                <li><strong>Guarantees Global Minimum:</strong> The optimization problem is convex, ensuring a unique global optimum.</li>
                            </ul>
                        </div>
                        <div>
                            <h3 class="text-lg font-semibold mb-2 text-red-600">Cons</h3>
                            <ul class="list-disc list-inside text-slate-600 space-y-1">
                                <li><strong>Computationally Intensive:</strong> Can be slow to train on large datasets.</li>
                                <li><strong>Sensitive to Hyperparameter Tuning:</strong> Performance depends sharply on the choice of kernel, C (regularization), and $\epsilon$ (epsilon).</li>
                                <li><strong>Less Interpretable:</strong> The transformation to a high-dimensional space makes it difficult to interpret the model's coefficients directly.</li>
                                <li><strong>Does Not Provide Native Forecast Intervals:</strong> Lacks built-in uncertainty estimates, unlike some statistical models. [19]</li>
                                <li><strong>Requires Feature Engineering:</strong> Needs manual creation of lagged and time-based features.</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>Example Implementation</h2>
                    <p>Here's an example of implementing SVR for time series forecasting in Python using `scikit-learn`. The process involves creating lagged features to transform the time series into a supervised learning problem, scaling the data, and then training an `SVR` model.</p>

                    <h3>Python Example (using `scikit-learn` library)</h3>
                    <div class="code-block my-4">
                        <pre class="bg-yellow-100 rounded p-4 text-sm mb-2 overflow-x-auto"><code class="language-python">
import pandas as pd
import numpy as np
from sklearn.svm import SVR
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt

# 1. Create sample time series data
date_range = pd.date_range(start='2020-01-01', periods=300, freq='D')
# Simulate data with trend, seasonality, and noise
values = (100 + np.arange(300) * 0.5 + # Trend
          20 * np.sin(np.arange(300) * 2 * np.pi / 30) + # Monthly seasonality
          np.random.randn(300) * 5) # Noise
df = pd.DataFrame({'date': date_range, 'value': values})

# 2. Feature Engineering (Create lagged features) [18]
def create_lagged_features(df, lag_steps):
    for i in range(1, lag_steps + 1):
        df[f'lag_{i}'] = df['value'].shift(i)
    return df

df = create_lagged_features(df.copy(), 7) # Use last 7 days as features

# Drop rows with NaN values created by lagging
df = df.dropna()

# 3. Splitting Data (Chronological Split)
split_date = '2020-09-01'
train = df[df['date'] < split_date]
test = df[df['date'] >= split_date]

features = [col for col in df.columns if col not in ['date', 'value']]
target = 'value'

X_train, y_train = train[features], train[target]
X_test, y_test = test[features], test[target]

# 4. Create a pipeline with scaling and SVR [18]
# StandardScaler is often crucial for SVR
# kernel: 'rbf' (Radial Basis Function) is common for non-linear data
# C: Regularization parameter. Higher C means less regularization.
# epsilon: Epsilon-tube parameter. Errors within this margin are ignored.
model_svr = make_pipeline(StandardScaler(), SVR(kernel='rbf', C=1.0, epsilon=0.1))

# 5. Fit the SVR model
model_svr.fit(X_train, y_train)

# 6. Make Predictions
predictions = model_svr.predict(X_test)

# 7. Evaluate Model Performance
mae = mean_absolute_error(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))
print(f"MAE: {mae:.3f}")
print(f"RMSE: {rmse:.3f}")

# 8. Plotting Results
plt.figure(figsize=(14, 7))
plt.plot(train['date'], train['value'], label='Training Data', color='blue')
plt.plot(test['date'], y_test, label='Actual Test Data', color='orange')
plt.plot(test['date'], predictions, label='SVR Predictions', color='green', linestyle='--')
plt.title('SVR Time Series Forecasting')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.grid(True)
plt.show()
                        </code></pre>
                    </div>
                </section>
                
                <section>
                    <h2>Dependencies & Resources</h2>
                    <p><strong>Dependencies:</strong> <code>pandas</code>, <code>numpy</code>, <code>scikit-learn</code>, <code>matplotlib</code> (for plotting).</p>
                    <ul class="list-disc list-inside space-y-2">
                        <li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html" target="_blank" rel="noopener noreferrer">Scikit-learn SVR Documentation ‚Üó</a></li>
                        <li><a href="https://stackoverflow.com/questions/41311017/time-series-forecasting-with-svr-in-scikit-learn" target="_blank" rel="noopener noreferrer">Time series forecasting with SVR in scikit learn (Stack Overflow) ‚Üó</a> [18]</li>
                        <li><a href="https://www.researchgate.net/publication/222076085_Localized_support_vector_regression_for_time_series_prediction" target="_blank" rel="noopener noreferrer">Localized support vector regression for time series prediction (ResearchGate) ‚Üó</a> [20]</li>
                        <li><a href="https://laurentlsantos.github.io/forecasting/support-vector-regression.html" target="_blank" rel="noopener noreferrer">Support Vector Regression for Time Series Forecasting (Blog) ‚Üó</a> [21]</li>
                        <li><a href="http://ieomsociety.org/proceedings/2021indonesia/574.pdf" target="_blank" rel="noopener noreferrer">Support Vector Regression for Forecasting Cases (PDF) ‚Üó</a> [22]</li>
                        <li><a href="https://www.researchgate.net/publication/220219323_Time_series_forecasting_by_a_seasonal_support_vector_regression_model" target="_blank" rel="noopener noreferrer">Time series forecasting by a seasonal support vector regression model (ResearchGate) ‚Üó</a> [19]</li>
                    </ul>
                </section>
            </article>
        </div>
    </main>

    <footer class="text-center p-6 bg-slate-200 mt-12">
        <p class="text-slate-600 text-sm">TSM Hub: An Interactive Resource for Time Series Analysis</p>
    </footer>

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$','$'], ['\\(','\\)']],
                displayMath: [['$$','$$'], ['\\[','\\]']],
                processEscapes: true,
                ignoreHtmlClass: ".*",
                processHtmlClass: "mathjax-process"
            },
            skipStartupTypeset: true
        });
    </script>
    <script src="main.js"></script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script>
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof MathJax!== 'undefined') {
                MathJax.Hub.Queue();
            }
        });
    </script>
</body>
</html>
