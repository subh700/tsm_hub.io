<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>XGBoost Model - TSM Hub</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
</head>
<body class="bg-slate-100 text-slate-800">
<!-- Enhanced Navigation for Model Pages -->
<nav class="bg-white shadow-lg sticky top-0 z-50 mb-8">
    <div class="container mx-auto px-6 py-4">
        <div class="flex justify-between items-center">
            <div class="flex items-center space-x-4">
                <a href="index.html" class="text-2xl font-bold text-blue-600">üè† TSF Hub</a>
                <div class="hidden md:flex items-center space-x-1 bg-gray-100 rounded-lg p-1">
                    <a href="models.html" class="nav-link px-3 py-2 rounded">üìö All Models</a>
                    <a href="comparison.html" class="nav-link px-3 py-2 rounded">üìä Compare</a>
                </div>
            </div>
            
            <div class="flex items-center space-x-3">
                <!-- AI Features Quick Access -->
                <div class="hidden md:flex items-center space-x-2">
                    <a href="model-selector.html" class="bg-blue-500 text-white px-3 py-2 rounded-lg hover:bg-blue-600 text-sm flex items-center">
                        ü§ñ AI Recommender
                    </a>
                    <a href="chat.html" class="bg-green-500 text-white px-3 py-2 rounded-lg hover:bg-green-600 text-sm flex items-center">
                        üí¨ AI Assistant
                    </a>
                    <a href="forecast-simulator.html?model={{MODEL_NAME}}" class="bg-purple-500 text-white px-3 py-2 rounded-lg hover:bg-purple-600 text-sm flex items-center">
                        üöÄ Try in Simulator
                    </a>
                </div>
                
                <!-- Mobile menu button -->
                <button class="md:hidden p-2" onclick="toggleMobileMenu()">‚ò∞</button>
            </div>
        </div>
        
        <!-- Mobile Menu -->
        <div id="mobile-menu" class="hidden md:hidden mt-4 pb-4 border-t">
            <div class="flex flex-col space-y-2">
                <a href="models.html" class="text-gray-600 hover:text-blue-600 py-2">üìö All Models</a>
                <a href="comparison.html" class="text-gray-600 hover:text-blue-600 py-2">üìä Compare Models</a>
                <a href="model-selector.html" class="text-gray-600 hover:text-blue-600 py-2">ü§ñ AI Recommender</a>
                <a href="chat.html" class="text-gray-600 hover:text-blue-600 py-2">üí¨ AI Assistant</a>
                <a href="forecast-simulator.html" class="text-gray-600 hover:text-blue-600 py-2">üöÄ Simulator</a>
            </div>
        </div>
    </div>
</nav>

    <header class="bg-white shadow-md sticky top-0 z-50">
        <nav class="container mx-auto px-6 py-4 flex justify-between items-center">
            <a href="index.html" class="text-2xl font-bold text-slate-900">TSM Hub</a>
            <div class="hidden md:flex space-x-6 items-center">
                <a href="index.html" class="nav-link">Home</a>
                <a href="concepts.html" class="nav-link">Concepts</a>
                <a href="models.html" class="nav-link active">Models</a>
                <a href="comparison.html" class="nav-link">Comparison</a>
            </div>
            <button id="mobile-menu-button" class="md:hidden p-2">
                <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7" />
                </svg>
            </button>
        </nav>
        <div id="mobile-menu" class="hidden md:hidden">
            <a href="index.html" class="block py-2 px-4 text-sm hover:bg-slate-200">Home</a>
            <a href="concepts.html" class="block py-2 px-4 text-sm hover:bg-slate-200">Concepts</a>
            <a href="models.html" class="block py-2 px-4 text-sm hover:bg-slate-200">Models</a>
            <a href="comparison.html" class="block py-2 px-4 text-sm hover:bg-slate-200">Comparison</a>
        </div>
    </header>

    <main class="container mx-auto p-4 md:p-8">
        <div class="bg-white p-8 rounded-lg shadow-md">
            <header class="border-b-2 border-slate-200 pb-6 mb-8">
                 <a href="models.html" class="text-blue-600 hover:underline mb-4 inline-block">&larr; Back to Model Library</a>
                <h1 class="text-4xl md:text-5xl font-bold text-slate-900">XGBoost Model</h1>
                <p class="mt-2 text-lg text-slate-600">eXtreme Gradient Boosting</p>
            </header>
            
            <article class="prose max-w-none">
                <section>
                    <h2>Overview</h2>
                    <p>XGBoost (eXtreme Gradient Boosting) is an optimized, distributed gradient boosting library designed to be highly efficient, flexible, and portable. It is a powerful machine learning algorithm that has achieved state-of-the-art results on many structured data problems and is widely used in Kaggle competitions. While primarily designed for tabular data, XGBoost can be effectively adapted for time series forecasting by transforming the time series problem into a supervised learning problem through **feature engineering**.</p>
                    
                    <h2>Architecture & Components</h2>
                    <p>XGBoost is an ensemble learning method that builds a prediction model in the form of an ensemble of weak prediction models, typically decision trees. Its core principles are:</p>
                    <ul>
                        <li><strong>Gradient Boosting:</strong> It builds trees sequentially, where each new tree is trained to predict the residuals (errors) of the previous ensemble. The predictions of all trees are then summed up to make the final prediction. This iterative error correction leads to a strong predictive model.</li>
                        <li><strong>Regularization:</strong> XGBoost includes regularization terms (L1 and L2) in its objective function to prevent overfitting. This helps in controlling model complexity and improving generalization.</li>
                        <li><strong>Parallelization:</strong> While boosting is sequential, XGBoost optimizes the tree construction process by parallelizing the computation of splits across features and data instances.</li>
                        <li><strong>Handling Missing Values:</strong> XGBoost can inherently handle missing values by learning the best direction for splits when data is missing.</li>
                        <li><strong>Feature Engineering:</strong> For time series forecasting, XGBoost relies on manually engineered features to capture temporal patterns. These typically include:
                            <ul>
                                <li><strong>Lagged Features:</strong> Past values of the time series itself (e.g., sales from previous days). [8]</li>
                                <li><strong>Rolling Window Statistics:</strong> Mean, standard deviation, min, max over a defined past window (e.g., 3-day, 7-day, 14-day moving averages). [5]</li>
                                <li><strong>Time-Based Features:</strong> Day of week, month, year, hour, quarter, day of year, week of year, and holiday indicators. [9]</li>
                                <li><strong>Decomposition:</strong> Explicitly decomposing the time series into trend, seasonality, and residuals, and then using these components as features or training XGBoost on the residuals. [8]</li>
                            </ul>
                        </li>
                    </ul>
                     <div class="bg-slate-100 p-4 rounded-lg my-6 text-center">
                        <img src="https://placehold.co/600x300/e2e8f0/334155?text=Conceptual+XGBoost+for+Time+Series" alt="XGBoost Architecture Diagram" class="mx-auto rounded-md">
                        <p class="text-sm text-slate-500 mt-2">Conceptual diagram of XGBoost's tree-based ensemble approach for time series.</p>
                    </div>
                </section>
                
                <section>
                    <h2>When to Use XGBoost</h2>
                    <p>XGBoost is a powerful choice for time series forecasting when:</p>
                    <ul>
                        <li><strong>High performance and accuracy are required:</strong> It consistently achieves excellent results in various machine learning tasks.</li>
                        <li><strong>You are comfortable with feature engineering:</strong> Its effectiveness in time series relies on creating relevant temporal features.</li>
                        <li><strong>The time series exhibits complex non-linear relationships:</strong> Tree-based models can capture intricate interactions between features.</li>
                        <li><strong>You have large datasets:</strong> It is optimized for speed and scalability.</li>
                        <li><strong>You need to model both trend and seasonality:</strong> Through appropriate feature engineering or decomposition.</li>
                        <li><strong>Robustness to missing values is important:</strong> It can handle missing data inherently.</li>
                    </ul>
                </section>

                <section>
                    <h2>Pros and Cons</h2>
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                        <div>
                            <h3 class="text-lg font-semibold mb-2 text-green-600">Pros</h3>
                            <ul class="list-disc list-inside text-slate-600 space-y-1">
                                <li><strong>High Performance & Accuracy:</strong> Often achieves state-of-the-art results on tabular data.</li>
                                <li><strong>Fast & Efficient:</strong> Optimized for speed and can handle large datasets.</li>
                                <li><strong>Flexible:</strong> Supports various objective functions and evaluation metrics.</li>
                                <li><strong>Handles Missing Values:</strong> Can inherently handle missing data.</li>
                                <li><strong>Built-in Regularization:</strong> Helps prevent overfitting.</li>
                                <li><strong>Provides Feature Importance:</strong> Can identify which features are most influential in predictions.</li>
                            </ul>
                        </div>
                        <div>
                            <h3 class="text-lg font-semibold mb-2 text-red-600">Cons</h3>
                            <ul class="list-disc list-inside text-slate-600 space-y-1">
                                <li><strong>Requires Feature Engineering:</strong> Not a native time series model; requires manual creation of lagged, rolling, and time-based features. [8, 10]</li>
                                <li><strong>Struggles with Extrapolation:</strong> As a tree-based model, it cannot predict values outside the range seen in the training data, making it less suitable for strong trends that extend far beyond the observed data. [11]</li>
                                <li><strong>Less Suited for Long-Term Dependencies:</strong> While features can capture some temporal context, it's less inherently designed for very long-term dependencies compared to RNNs or Transformers.</li>
                                <li><strong>Prone to Overfitting:</strong> Can overfit if not tuned carefully, especially with many features.</li>
                                <li><strong>Computational Cost:</strong> Can be slower than LightGBM on very large datasets due to its level-wise tree growth.</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>Example Implementation</h2>
                    <p>Here's an example of implementing XGBoost for time series forecasting in Python. The key steps involve feature engineering, splitting data chronologically, and then training and predicting with `XGBRegressor`.</p>

                    <h3>Python Example (using `xgboost` library)</h3>
                    <div class="code-block my-4">
                        <pre><code class="language-python">
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt

# 1. Create sample time series data
date_range = pd.date_range(start='2020-01-01', periods=300, freq='D')
# Simulate data with trend, seasonality, and noise
values = (100 + np.arange(300) * 0.5 + # Trend
          20 * np.sin(np.arange(300) * 2 * np.pi / 30) + # Monthly seasonality
          np.random.randn(300) * 5) # Noise
df = pd.DataFrame({'date': date_range, 'value': values})

# 2. Feature Engineering [8, 9]
# Lagged features
df['lag_1'] = df['value'].shift(1)
df['lag_7'] = df['value'].shift(7) # Weekly lag

# Rolling window features
df['rolling_mean_7'] = df['value'].rolling(window=7).mean().shift(1) # Shifted to avoid data leakage
df['rolling_std_7'] = df['value'].rolling(window=7).std().shift(1)

# Time-based features
df['day_of_week'] = df['date'].dt.dayofweek
df['month'] = df['date'].dt.month
df['year'] = df['date'].dt.year
df['day_of_year'] = df['date'].dt.dayofyear
df['is_weekend'] = df['day_of_week'].isin([1, 2]).astype(int)

# Drop rows with NaN values created by lagging/rolling features
df = df.dropna()

# 3. Splitting Data (Chronological Split) [9]
split_date = '2020-09-01'
train = df[df['date'] < split_date]
test = df[df['date'] >= split_date]

features = [col for col in df.columns if col not in ['date', 'value']]
target = 'value'

X_train, y_train = train[features], train[target]
X_test, y_test = test[features], test[target]

# 4. Create and Train XGBoost Model [9]
# n_estimators: number of boosting rounds
# early_stopping_rounds: stop if validation metric doesn't improve for this many rounds
reg = xgb.XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=5, random_state=42)
reg.fit(X_train, y_train,
        eval_set=[(X_train, y_train), (X_test, y_test)],
        early_stopping_rounds=50, # Stop if no improvement for 50 rounds
        verbose=False) # Set to True for verbose output during training

# 5. Make Predictions [9]
predictions = reg.predict(X_test)

# 6. Evaluate Model Performance [9]
mae = mean_absolute_error(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))
print(f"MAE: {mae:.3f}")
print(f"RMSE: {rmse:.3f}")

# 7. Plotting Results [9]
plt.figure(figsize=(14, 7))
plt.plot(train['date'], train['value'], label='Training Data', color='blue')
plt.plot(test['date'], y_test, label='Actual Test Data', color='orange')
plt.plot(test['date'], predictions, label='XGBoost Predictions', color='green', linestyle='--')
plt.title('XGBoost Time Series Forecasting')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.grid(True)
plt.show()
                        </code></pre>
                    </div>
                </section>
                
                <section>
                    <h2>Dependencies & Resources</h2>
                    <p><strong>Dependencies:</strong> <code>pandas</code>, <code>numpy</code>, <code>xgboost</code>, <code>scikit-learn</code> (for metrics), <code>matplotlib</code> (for plotting).</p>
                    <ul class="list-disc list-inside space-y-2">
                        <li><a href="https://xgboost.readthedocs.io/en/stable/" target="_blank" rel="noopener noreferrer">XGBoost Official Documentation ‚Üó</a></li>
                        <li><a href="https://github.com/dmlc/xgboost" target="_blank" rel="noopener noreferrer">XGBoost GitHub Repository ‚Üó</a></li>
                        <li><a href="https://machinelearningmastery.com/xgboost-for-time-series-forecasting/" target="_blank" rel="noopener noreferrer">XGBoost for Time Series Forecasting (Machine Learning Mastery) ‚Üó</a> [10]</li>
                        <li><a href="https://www.kaggle.com/code/furiousx7/xgboost-time-series" target="_blank" rel="noopener noreferrer">XGBoost Time Series (Kaggle Notebook) ‚Üó</a> [9]</li>
                        <li><a href="https://medium.com/@byashwanth77/forecasting-using-xgboost-lag-and-decomposition-864815bd98c5" target="_blank" rel="noopener noreferrer">Forecasting Using Xgboost (lag and decomposition) (Medium Blog) ‚Üó</a> [8]</li>
                        <li><a href="https://www.kaggle.com/code/youknowjp/time-series-forcasting-using-xgboost" target="_blank" rel="noopener noreferrer">Time Series Forecasting using XGBoost Model (Kaggle Notebook) ‚Üó</a> [12]</li>
                    </ul>
                </section>
            </article>
        </div>
    </main>

    <footer class="text-center p-6 bg-slate-200 mt-12">
        <p class="text-slate-600 text-sm">TSM Hub: An Interactive Resource for Time Series Analysis</p>
    </footer>

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$','$'], ['\\(','\\)']],
                displayMath: [['$$','$$'], ['\\[','\\]']],
                processEscapes: true,
                ignoreHtmlClass: ".*",
                processHtmlClass: "mathjax-process"
            },
            skipStartupTypeset: true
        });
    </script>
    <script src="main.js"></script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script>
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof MathJax!== 'undefined') {
                MathJax.Hub.Queue();
            }
        });
    </script>
</body>
</html>
